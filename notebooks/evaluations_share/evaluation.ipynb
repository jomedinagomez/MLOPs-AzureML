{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3870d4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "def configure_logging(level=\"ERROR\"):\n",
    "    try:\n",
    "        ## Convert the level string to uppercase so it matches what the logging library expects\n",
    "        logging_level = getattr(logging, level.upper(), None)\n",
    "        ## Validate that the level is a valid logging level\n",
    "        if not isinstance(logging_level, int):\n",
    "            raise ValueError(f'Invalid log level: {level}')\n",
    "        ## Setup a logging format\n",
    "        logging.basicConfig(\n",
    "            level=logging_level,\n",
    "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "            handlers=[logging.StreamHandler(sys.stdout)]\n",
    "        )\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to set up logging: {e}\", file=sys.stderr)\n",
    "        sys.exit(1)\n",
    "\n",
    "configure_logging(level=\"DEBUG\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9b636f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# Required environment variables:\n",
    "endpoint = os.environ[\"PROJECT_ENDPOINT\"] # https://<account>.services.ai.azure.com/api/projects/<project>\n",
    "model_endpoint = os.environ[\"MODEL_ENDPOINT\"] # https://<account>.services.ai.azure.com\n",
    "model_api_key = os.environ[\"MODEL_API_KEY\"]\n",
    "model_deployment_name = os.environ[\"MODEL_DEPLOYMENT_NAME\"] # E.g. gpt-4o-mini\n",
    "\n",
    "# Optional: Reuse an existing dataset.\n",
    "dataset_name    = os.environ.get(\"DATASET_NAME\",    \"dataset-test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821ab98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, secrets, string\n",
    "\n",
    "def generate_dataset_version(length: int = 8, prefix: str = \"v\") -> str:\n",
    "    alphabet = string.ascii_letters + string.digits\n",
    "    return prefix + \"\".join(secrets.choice(alphabet) for _ in range(length))\n",
    "\n",
    "dataset_version = generate_dataset_version()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4effe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from azure.identity import DefaultAzureCredential\n",
    "from azure.ai.projects import AIProjectClient\n",
    "\n",
    "# Create the project client (Foundry project and credentials):\n",
    "project_client = AIProjectClient(\n",
    "    endpoint=endpoint,\n",
    "    credential=DefaultAzureCredential(),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c9b5cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload a local JSONL file. Skip this step if you already have a dataset registered.\n",
    "data_id = project_client.datasets.upload_file(\n",
    "    name=dataset_name,\n",
    "    version=dataset_version,\n",
    "    file_path=\"./evaluate_test_data.jsonl\",\n",
    ").id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcc5ec32",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    EvaluatorConfiguration,\n",
    "    EvaluatorIds,\n",
    ")\n",
    "\n",
    "# Built-in evaluator configurations:\n",
    "evaluators = {\n",
    "    \"relevance\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.RELEVANCE.value,\n",
    "        init_params={\"deployment_name\": model_deployment_name},\n",
    "        data_mapping={\n",
    "            \"query\": \"${data.query}\",\n",
    "            \"response\": \"${data.response}\",\n",
    "        },\n",
    "    ),\n",
    "    \"violence\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.VIOLENCE.value,\n",
    "        init_params={\"azure_ai_project\": endpoint},\n",
    "    ),\n",
    "    \"bleu_score\": EvaluatorConfiguration(\n",
    "        id=EvaluatorIds.BLEU_SCORE.value,\n",
    "    ),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4afa36f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.projects.models import (\n",
    "    Evaluation,\n",
    "    InputDataset\n",
    ")\n",
    "\n",
    "# Create an evaluation with the dataset and evaluators specified.\n",
    "evaluation = Evaluation(\n",
    "    display_name=\"Cloud evaluation\",\n",
    "    description=\"Evaluation of dataset\",\n",
    "    data=InputDataset(id=data_id),\n",
    "    evaluators=evaluators,\n",
    ")\n",
    "\n",
    "# Run the evaluation.\n",
    "evaluation_response = project_client.evaluations.create(\n",
    "    evaluation,\n",
    "    headers={\n",
    "        \"model-endpoint\": model_endpoint,\n",
    "        \"api-key\": model_api_key,\n",
    "    },\n",
    ")\n",
    "\n",
    "print(\"Created evaluation:\", evaluation_response.name)\n",
    "print(\"Status:\", evaluation_response.status)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c0513f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlops-azureml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
