
$schema: https://azuremlschemas.azureedge.net/latest/pipelineJob.schema.json
type: pipeline
experiment_name: nyc-taxi-pipeline-class-prod
description: Train Classification model based on nyc taxi dataset for the production environment

# Notes:
# - These jobs currently use MLflow models. Later you can switch to a custom artifact flow.
# - To keep inputs minimal, we derive names in jobs and add a short artifact identifier:
#   artifact_id: 'm' for MLflow (now), 'c' for custom (later).
# - We apply artifact_id only to register/deploy names so training/comparison remain unchanged.
# - To enforce that test jobs run after deploy jobs, we use a dummy output (deploy_status) in deploy jobs
#   and a corresponding dummy input in test jobs. This does not transfer data, but ensures correct job order.
#   No code changes are needed for these dummy outputs/inputs.
#
# Patterns for environment-dependent inputs:
# - environment: 'dev' or 'prod' (controls the environment context for the pipeline)
# - automl_compute: 'aml-cluster-dev-cc01' for dev, 'aml-cluster-prod-cc01' for prod (set to your actual compute names)
# - registry: 'mlrdevcc01' for dev, 'mlrprodcc01' for prod (set to your actual registry names)
# - deployment_name: can be 'green', 'blue', or any deployment slot name; may differ between dev and prod
# - default_compute: should match automl_compute for the environment
#
# To switch between dev and prod, update these values accordingly in the inputs and settings sections.
#
# Workspace separation and naming pattern:
# - Dev and prod pipelines run in completely separate Azure ML workspaces, each in its own resource group.
# - Workspace resource group names follow the pattern: 'rg-<prefix>-ws-<env>-<location_code>-<naming_suffix>'
#   - Example (dev):   rg-aml-ws-dev-cc-01
#   - Example (prod):  rg-aml-ws-prod-cc-01
# - Workspace names follow the pattern: '<workspace_prefix>-<env>-<location_code>-<naming_suffix>'
#   - Example (dev):   mlw-dev-cc-01
#   - Example (prod):  mlw-prod-cc-01
# - The <prefix>, <workspace_prefix>, <location_code>, and <naming_suffix> are set in Terraform variables (see infra/variables.tf).
# - This ensures strict separation of dev and prod environments, with no resource overlap., 
# - When running the pipeline, always use the correct workspace for the target environment.


# <inputs_and_outputs>
inputs:
  # environment identifier for this pipeline config
  environment: 'prod'
  # model name
  model_base: 'taxi-class'
  # artifact identifier for naming: 'm' (MLflow now), 'c' (custom later)
  artifact_id: 'm'
  deployment_name: 'green'
  registry: 'mlrprodcc01'  # <-- set to your production registry
  # compute cluster for automl job
  automl_compute: 'aml-cluster-prod-cc01'  # <-- set to your production compute
  # conda file for the environment to be created
  conda_file: custom_prod.yaml
outputs: 
  pipeline_job_trained_model:
    mode: rw_mount
  pipeline_job_predictions:
    mode: rw_mount
  pipeline_job_score_report:
    mode: rw_mount
  pipeline_job_comparison:
    mode: rw_mount

# </inputs_and_outputs>

settings:
  default_datastore: azureml:workspaceblobstore
  # All jobs use this compute unless overridden at the job level. To change, edit this value.
  default_compute: azureml:aml-cluster-prod-cc01  # <-- set to your production compute
  continue_on_step_failure: false

jobs:
  merge_job:
    type: command
    code: ../src/merge_data
    command: >-
      python merge_data.py
      --raw_data_green ${{inputs.raw_data_green}}
      --raw_data_yellow ${{inputs.raw_data_yellow}}
      --merged_data ${{outputs.merged_data}}
    inputs:
      raw_data_green:
        type: uri_file 
        path: ../data/taxi-data/raw/greenTaxiData.csv
      raw_data_yellow:
        type: uri_file 
        path: ../data/taxi-data/raw/yellowTaxiData.csv
    outputs:
      merged_data:
        mode: upload
    environment: azureml:AzureML-sklearn-1.0-ubuntu20.04-py38-cpu@latest

  transform_job:
    type: command
    component: ../src/components/transform.yaml
    inputs:
      test_split_ratio: 0.3
      clean_data: ${{parent.jobs.merge_job.outputs.merged_data}}
    outputs:
      train_data:
      test_data: 

  train_job:
    type: command
    component: ../src/components/train.yaml
    inputs:
      training_data: ${{parent.jobs.transform_job.outputs.train_data}}
      testing_data: ${{parent.jobs.transform_job.outputs.test_data}}
      model_name: '${{parent.inputs.model_base}}-${{parent.inputs.environment}}'
      automl_compute: ${{parent.inputs.automl_compute}}
    outputs:
      model_output: ${{parent.outputs.pipeline_job_trained_model}}

  predict_job:
    type: command
    component: ../src/components/predict.yaml
    inputs:
      model_input: ${{parent.jobs.train_job.outputs.model_output}}
      test_data: ${{parent.jobs.transform_job.outputs.test_data}}
    outputs:
      predictions: ${{parent.outputs.pipeline_job_predictions}}

  compare_job:
    type: command
    component: ../src/components/compare.yaml
    inputs:
      model_input: ${{parent.jobs.train_job.outputs.model_output}}
      test_data: ${{parent.jobs.transform_job.outputs.test_data}}
      predictions: ${{parent.jobs.predict_job.outputs.predictions}}
      model_name: '${{parent.inputs.model_base}}-${{parent.inputs.environment}}'
    outputs:
      compare_output: ${{parent.outputs.pipeline_job_comparison}}
    
  score_job:
    type: command
    component: ../src/components/score.yaml
    inputs:
      predictions: ${{parent.jobs.predict_job.outputs.predictions}}
      model: ${{parent.jobs.train_job.outputs.model_output}}
    outputs:
      score_report: ${{parent.outputs.pipeline_job_score_report}}

  registerws_job:
    type: command
    component: ../src/components/register.yaml
    inputs:
      model_input: ${{parent.jobs.train_job.outputs.model_output}}
      # model name with artifact identifier (MLflow='m', custom='c')
      model_name: '${{parent.inputs.model_base}}-${{parent.inputs.environment}}-${{parent.inputs.artifact_id}}'
      compare_output: ${{parent.jobs.compare_job.outputs.compare_output}}
    outputs:
      register_output:

  registerex_job:
    type: command
    component: ../src/components/register.yaml
    inputs:
      model_input: ${{parent.jobs.train_job.outputs.model_output}}
      # model name with artifact identifier (MLflow='m', custom='c')
      model_name: '${{parent.inputs.model_base}}-${{parent.inputs.environment}}-${{parent.inputs.artifact_id}}'
      registry: ${{parent.inputs.registry}}
      compare_output: ${{parent.jobs.compare_job.outputs.compare_output}}
    outputs:
      register_output:

  deployex_job:
    type: command
    component: ../src/components/deploy.yaml
    inputs:
      # names include artifact_id (MLflow='m', custom='c') to distinguish implementations
      model_name: '${{parent.inputs.model_base}}-${{parent.inputs.environment}}-${{parent.inputs.artifact_id}}'
      endpoint_name: '${{parent.inputs.model_base}}-ex-${{parent.inputs.environment}}-${{parent.inputs.artifact_id}}-col'
      deployment_name: ${{parent.inputs.deployment_name}}
      registry: ${{parent.inputs.registry}}
      register_job_status: ${{parent.jobs.registerex_job.outputs.register_output}}
    # Dummy output to enforce dependency for test job. Not used for data transfer.
    outputs:
      deploy_status:  # This output is only used to enforce that test_endpoint_ex_job runs after deployex_job

  deployws_job:
    type: command
    component: ../src/components/deploy.yaml
    inputs:
      # names include artifact_id (MLflow='m', custom='c') to distinguish implementations
      model_name: '${{parent.inputs.model_base}}-${{parent.inputs.environment}}-${{parent.inputs.artifact_id}}'
      endpoint_name: '${{parent.inputs.model_base}}-ws-${{parent.inputs.environment}}-${{parent.inputs.artifact_id}}-col'
      deployment_name: ${{parent.inputs.deployment_name}}
      register_job_status: ${{parent.jobs.registerws_job.outputs.register_output}}
    # Dummy output to enforce dependency for test job. Not used for data transfer.
    outputs:
      deploy_status:  # This output is only used to enforce that test_endpoint_ws_job runs after deployws_job

  test_endpoint_ex_job:
    type: command
    component: ../src/components/test_endpoint.yaml
    inputs:
      endpoint_name: '${{parent.inputs.model_base}}-ex-${{parent.inputs.environment}}-${{parent.inputs.artifact_id}}-col'
      deployment_name: ${{parent.inputs.deployment_name}}
      test_data: ${{parent.jobs.transform_job.outputs.test_data}}
      # This input is a dummy dependency to ensure this job runs after deployex_job
      deploy_status: ${{parent.jobs.deployex_job.outputs.deploy_status}}
    outputs:
      report_folder:

  test_endpoint_ws_job:
    type: command
    component: ../src/components/test_endpoint.yaml
    inputs:
      endpoint_name: '${{parent.inputs.model_base}}-ws-${{parent.inputs.environment}}-${{parent.inputs.artifact_id}}-col'
      deployment_name: ${{parent.inputs.deployment_name}}
      test_data: ${{parent.jobs.transform_job.outputs.test_data}}
      # This input is a dummy dependency to ensure this job runs after deployws_job
      deploy_status: ${{parent.jobs.deployws_job.outputs.deploy_status}}
    outputs:
      report_folder:

  create_env_job:
    type: command
    component: ../src/components/create_env.yaml
    inputs:
      env_name: taxi-class-env-prod
      conda_file: ${{parent.inputs.conda_file}}
    outputs:
      env_status:
    # This job creates the environment and can be used as a dependency for other jobs if needed
